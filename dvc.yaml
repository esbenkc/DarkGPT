stages:
  make_prompts:
    cmd: >-
      python scripts/make_prompts.py
      --prompts-file=data/raw.csv
      --output-dir=data/prompts
    deps:
      - darkgpt/models/base.py
      - data/raw.csv
      - poetry.lock
      - scripts/make_prompts.py
    outs:
      - data/prompts

  generate_conversations:
    matrix:
      assistant: ${assistants}
      prompt_type: ${prompt_types}
    cmd: >-
      python scripts/generate_conversations.py
      --model=${item.assistant}
      --params-file=params.yaml
      --prompts-dir=data/prompts/${item.prompt_type}
      --output-dir=data/conversations/${item.assistant}/${item.prompt_type}
      --limit=${limit}
    deps:
      - darkgpt/models
      - data/prompts/${item.prompt_type}
      - poetry.lock
      - scripts/generate_conversations.py
    params:
      - models.${item.assistant}.build
    outs:
      - data/conversations/${item.assistant}/${item.prompt_type}

  run_eval:
    matrix:
      assistant: ${assistants}
      prompt_type: ${prompt_types}
      overseer: ${eval.overseers}
    cmd: >-
      python scripts/run_eval.py
      --overseer=${item.overseer}
      --assistant=${item.assistant}
      --conversations-dir=data/conversations/${item.assistant}/${item.prompt_type}
      --output-dir=data/evals/${item.assistant}/${item.prompt_type}/${item.overseer}
      --params-file=params.yaml
    deps:
      - darkgpt
      - data/conversations/${item.assistant}/${item.prompt_type}
      - poetry.lock
      - scripts/run_eval.py
    params:
      - eval.issues
      - models.${item.assistant}.prompt_info
      - models.${item.overseer}.build
    outs:
      - data/evals/${item.assistant}/${item.prompt_type}/${item.overseer}

  summarize:
    matrix:
      assistant: ${assistants}
      prompt_type: ${prompt_types}
    cmd: >-
      python scripts/summarize.py
      --params_file=params.yaml
      --manifest=data/prompts/manifest.yaml:${item.prompt_type}
      --evals_dir=data/evals/${item.assistant}/${item.prompt_type}
      --output_file=metrics/${item.prompt_type}/${item.assistant}.yaml
    deps:
      - data/evals/${item.assistant}/${item.prompt_type}
      - data/prompts/manifest.yaml
      - poetry.lock
      - scripts/summarize.py
    params:
      - eval.overseers
    metrics:
      - metrics/${item.prompt_type}/${item.assistant}.yaml

  plot_issues:
    cmd: >-
      python scripts/plot_issues.py
      --metrics_dir=metrics
      --output_file=plots/issues_by_model_and_overseer.png
    deps:
      - metrics
      - poetry.lock
      - scripts/plot_issues.py
    outs:
      - plots/issues_by_model_and_overseer.png:
          cache: False

  sample_for_human_annotation:
    cmd: >-
      python scripts/sample_for_human_annotation.py
      --conversations-dir data/conversations
      --prompt-manifest-file data/prompts/manifest.yaml
      --template-file data/human/lsq.jinja
      --output-dir data/human/sampled
      --params-file params.yaml
    deps:
      - data/conversations
      - data/human/lsq.jinja
      - data/prompts/manifest.yaml
      - poetry.lock
      - scripts/sample_for_human_annotation.py
    params:
      - assistants
      - sample
    outs:
      - data/human/sampled
